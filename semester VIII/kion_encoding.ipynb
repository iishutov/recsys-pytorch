{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from spacy.lang.ru.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter = pd.read_csv('./mod_data/interactions.csv')\n",
    "df_items = pd.read_csv('./mod_data/items.csv')\n",
    "df_users = pd.read_csv('./mod_data/users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>age_rating</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>7314</td>\n",
       "      <td>film</td>\n",
       "      <td>Секреты Лагерфельда</td>\n",
       "      <td>документальное</td>\n",
       "      <td>16</td>\n",
       "      <td>дизайнер моды, Модельер, 2007, франция, секрет...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id content_type                title          genres  age_rating  \\\n",
       "9989     7314         film  Секреты Лагерфельда  документальное          16   \n",
       "\n",
       "                                               keywords  \n",
       "9989  дизайнер моды, Модельер, 2007, франция, секрет...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords extraction\n",
    "def tokenize(line):\n",
    "    vec = [word for word in line.split(', ') if not word.isnumeric() and word not in {'', 'nan'}]\n",
    "    return vec\n",
    "\n",
    "keywords_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize, token_pattern=None,\n",
    "    max_features=100, stop_words=list(STOP_WORDS)\n",
    ")\n",
    "X_keywords = keywords_vectorizer.fit_transform(df_items['keywords'].values.astype('U'))\n",
    "\n",
    "# content type preprocessing\n",
    "X_content_type = (df_items['content_type'] == 'film').astype(int).values.reshape(-1, 1)\n",
    "\n",
    "# age rating preprocessing\n",
    "age_rating_encoder = OneHotEncoder()\n",
    "X_age_rating = age_rating_encoder.fit_transform(df_items['age_rating'].values.astype('U').reshape(-1, 1)).toarray()\n",
    "\n",
    "# genres preprocessing\n",
    "genres_vectorizer = TfidfVectorizer(tokenizer=tokenize, token_pattern=None)\n",
    "X_genres = genres_vectorizer.fit_transform(df_items['genres'].values.astype('U')).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "россия                     2646.475781\n",
       "соединенные штаты          1650.497265\n",
       "отношения                  1344.449214\n",
       "франция                    1185.161904\n",
       "сша                        1085.736725\n",
       "ссср                        756.435969\n",
       "любовь                      633.931426\n",
       "дружба                      599.568120\n",
       "соединенное королевство     495.680623\n",
       "женщины                     490.716083\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data=np.squeeze(np.asarray(X_keywords.sum(axis=0))), index=keywords_vectorizer.get_feature_names_out())\\\n",
    "    .sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15963, 202)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_items = np.hstack([\n",
    "    X_content_type,\n",
    "    X_genres,\n",
    "    X_age_rating,\n",
    "    X_keywords.todense()\n",
    "])\n",
    "X_items.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>sex</th>\n",
       "      <th>kids_flg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457842</th>\n",
       "      <td>490793</td>\n",
       "      <td>age_18_24</td>\n",
       "      <td>income_20_40</td>\n",
       "      <td>Ж</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        age        income sex  kids_flg\n",
       "457842   490793  age_18_24  income_20_40   Ж         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age extraction\n",
    "age_encoder = OneHotEncoder()\n",
    "X_age = age_encoder.fit_transform(df_users['age'].values.astype('U').reshape(-1, 1)).toarray()\n",
    "\n",
    "# income extraction\n",
    "income_encoder = OneHotEncoder()\n",
    "X_income = income_encoder.fit_transform(df_users['income'].values.astype('U').reshape(-1, 1)).toarray()\n",
    "\n",
    "# sex extraction\n",
    "X_sex = (df_users['sex'] == 'М').astype(int).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840197, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_users = np.hstack([\n",
    "    X_age,\n",
    "    X_income,\n",
    "    X_sex,\n",
    "    df_users['kids_flg'].to_numpy().reshape(-1, 1)\n",
    "])\n",
    "X_users.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-2.1.0+cu121.html\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric import EdgeIndex\n",
    "from torch_geometric.loader import LinkNeighborLoader, NeighborLoader\n",
    "from torch_geometric.metrics import (\n",
    "    LinkPredMAP,\n",
    "    LinkPredPrecision,\n",
    "    LinkPredRecall,\n",
    ")\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION']='1'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_idx = {user_id: idx for idx, user_id in df_users['user_id'].items()}\n",
    "item_id_to_idx = {item_id: idx for idx, item_id in df_items['item_id'].items()}\n",
    "\n",
    "edges = np.vstack([\n",
    "    df_inter['user_id'].map(user_id_to_idx).values,\n",
    "    df_inter['item_id'].map(item_id_to_idx).values\n",
    "])\n",
    "\n",
    "connections = np.ones(edges.shape[1])\n",
    "time = pd.to_datetime(df_inter['last_watch_dt'], format='%Y-%m-%d').values.astype(np.int64) // 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  movie={ x=[15963, 202] },\n",
       "  user={ x=[840197, 14] },\n",
       "  (user, watched, movie)={\n",
       "    edge_index=[2, 1288996],\n",
       "    time=[1288996],\n",
       "  },\n",
       "  (movie, rev_watched, user)={\n",
       "    edge_index=[2, 1288996],\n",
       "    time=[1288996],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Add item node features\n",
    "data['movie'].x = torch.Tensor(X_items)\n",
    "# Add user node features for message passing\n",
    "data['user'].x = torch.Tensor(X_users)\n",
    "\n",
    "# Add edges (users connections with movies)\n",
    "data['user', 'watched', 'movie'].edge_index = torch.tensor(edges)\n",
    "data['user', 'watched', 'movie'].time = torch.tensor(time)\n",
    "\n",
    "# Add a reverse relation for message passing\n",
    "data = T.ToUndirected()(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:08<00:00, 31.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.2593\n",
      "Test MAP@10: 0.0483, Test Precision@10: 0.0229, Test Recall@10: 0.1542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:14<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Loss: 0.2409\n",
      "Test MAP@10: 0.0675, Test Precision@10: 0.0234, Test Recall@10: 0.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:20<00:00, 28.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Loss: 0.2369\n",
      "Test MAP@10: 0.0557, Test Precision@10: 0.0243, Test Recall@10: 0.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:19<00:00, 28.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Loss: 0.2338\n",
      "Test MAP@10: 0.0672, Test Precision@10: 0.0272, Test Recall@10: 0.1867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:19<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.2321\n",
      "Test MAP@10: 0.0581, Test Precision@10: 0.0240, Test Recall@10: 0.1638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:18<00:00, 29.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.2308\n",
      "Test MAP@10: 0.0846, Test Precision@10: 0.0276, Test Recall@10: 0.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:23<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Loss: 0.2296\n",
      "Test MAP@10: 0.0570, Test Precision@10: 0.0268, Test Recall@10: 0.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:15<00:00, 29.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Loss: 0.2288\n",
      "Test MAP@10: 0.0731, Test Precision@10: 0.0264, Test Recall@10: 0.1812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:16<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Loss: 0.2293\n",
      "Test MAP@10: 0.0723, Test Precision@10: 0.0252, Test Recall@10: 0.1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:18<00:00, 29.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2277\n",
      "Test MAP@10: 0.0602, Test Precision@10: 0.0273, Test Recall@10: 0.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:37<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.2274\n",
      "Test MAP@10: 0.0876, Test Precision@10: 0.0259, Test Recall@10: 0.1765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:12<00:00, 30.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.2270\n",
      "Test MAP@10: 0.0658, Test Precision@10: 0.0257, Test Recall@10: 0.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:12<00:00, 30.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.2269\n",
      "Test MAP@10: 0.0740, Test Precision@10: 0.0273, Test Recall@10: 0.1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:11<00:00, 30.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.2270\n",
      "Test MAP@10: 0.0716, Test Precision@10: 0.0246, Test Recall@10: 0.1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4029/4029 [02:11<00:00, 30.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.2268\n",
      "Test MAP@10: 0.0524, Test Precision@10: 0.0246, Test Recall@10: 0.1685\n"
     ]
    }
   ],
   "source": [
    "# Number of predictions\n",
    "k = 10\n",
    "\n",
    "# Perform a temporal link-level split into training and test edges:\n",
    "edge_label_index = data['user', 'movie'].edge_index\n",
    "time = data['user', 'movie'].time\n",
    "\n",
    "perm = time.argsort()\n",
    "train_index = perm[:int(0.8 * perm.numel())]\n",
    "test_index = perm[int(0.8 * perm.numel()):]\n",
    "\n",
    "kwargs = dict(  # Shared data loader arguments:\n",
    "    data=data,\n",
    "    num_neighbors=[5, 5, 5],\n",
    "    batch_size=256,\n",
    "    time_attr='time',\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    temporal_strategy='last',\n",
    ")\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    edge_label_index=(('user', 'movie'), edge_label_index[:, train_index]),\n",
    "    edge_label_time=time[train_index] - 1,  # No leakage.\n",
    "    neg_sampling=dict(mode='binary', amount=2),\n",
    "    shuffle=True,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "\n",
    "# During testing, we sample node-level subgraphs from both endpoints to\n",
    "# retrieve their embeddings.\n",
    "# This allows us to do efficient k-NN search on top of embeddings:\n",
    "src_loader = NeighborLoader(\n",
    "    input_nodes='user',\n",
    "    input_time=(time[test_index].min() - 1).repeat(data['user'].num_nodes),\n",
    "    **kwargs,\n",
    ")\n",
    "dst_loader = NeighborLoader(\n",
    "    input_nodes='movie',\n",
    "    input_time=(time[test_index].min() - 1).repeat(data['movie'].num_nodes),\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "# Save test edges and the edges we want to exclude when evaluating:\n",
    "sparse_size = (data['user'].num_nodes, data['movie'].num_nodes)\n",
    "test_edge_label_index = EdgeIndex(\n",
    "    edge_label_index[:, test_index].to(device),\n",
    "    sparse_size=sparse_size,\n",
    ").sort_by('row')[0]\n",
    "test_exclude_links = EdgeIndex(\n",
    "    edge_label_index[:, train_index].to(device),\n",
    "    sparse_size=sparse_size,\n",
    ").sort_by('row')[0]\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = gnn.SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv3 = gnn.SAGEConv((-1, -1), hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InnerProductDecoder(torch.nn.Module):\n",
    "    def forward(self, x_dict, edge_label_index):\n",
    "        x_src = x_dict['user'][edge_label_index[0]]\n",
    "        x_dst = x_dict['movie'][edge_label_index[1]]\n",
    "        return (x_src * x_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNN(hidden_channels)\n",
    "        self.encoder = gnn.to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = InnerProductDecoder()\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        x_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(x_dict, edge_label_index)\n",
    "\n",
    "\n",
    "model = Model(hidden_channels=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch['user', 'movie'].edge_label_index,\n",
    "        )\n",
    "        y = batch['user', 'movie'].edge_label\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * y.numel()\n",
    "        total_examples += y.numel()\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(edge_label_index, exclude_links):\n",
    "    model.eval()\n",
    "\n",
    "    dst_embs = []\n",
    "    for batch in dst_loader:  # Collect destination node/movie embeddings:\n",
    "        batch = batch.to(device)\n",
    "        emb = model.encoder(batch.x_dict, batch.edge_index_dict)['movie'].cpu()\n",
    "        emb = emb[:batch['movie'].batch_size]\n",
    "        dst_embs.append(emb)\n",
    "    dst_emb = torch.cat(dst_embs, dim=0)\n",
    "    del dst_embs\n",
    "\n",
    "    # Instantiate k-NN index based on maximum inner product search (MIPS):\n",
    "    mips = gnn.MIPSKNNIndex(dst_emb)\n",
    "\n",
    "    # Initialize metrics:\n",
    "    map_metric = LinkPredMAP(k=k)\n",
    "    precision_metric = LinkPredPrecision(k=k)\n",
    "    recall_metric = LinkPredRecall(k=k)\n",
    "\n",
    "    num_processed = 0\n",
    "    for batch in src_loader:  # Collect source node/user embeddings:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute user embeddings:\n",
    "        emb = model.encoder(batch.x_dict, batch.edge_index_dict)['user']\n",
    "        emb = emb[:batch['user'].batch_size]\n",
    "\n",
    "        # Filter labels/exclusion by current batch:\n",
    "        _edge_label_index = edge_label_index.sparse_narrow(\n",
    "            dim=0,\n",
    "            start=num_processed,\n",
    "            length=emb.size(0),\n",
    "        ).cpu()\n",
    "        _exclude_links = exclude_links.sparse_narrow(\n",
    "            dim=0,\n",
    "            start=num_processed,\n",
    "            length=emb.size(0),\n",
    "        ).cpu()\n",
    "        num_processed += emb.size(0)\n",
    "\n",
    "\n",
    "        # Perform MIPS search:\n",
    "        _, pred_index_mat = mips.search(emb.cpu(), k, _exclude_links)\n",
    "\n",
    "        # Update retrieval metrics:\n",
    "        map_metric.update(pred_index_mat, _edge_label_index)\n",
    "        precision_metric.update(pred_index_mat, _edge_label_index)\n",
    "        recall_metric.update(pred_index_mat, _edge_label_index)\n",
    "\n",
    "    return (\n",
    "        float(map_metric.compute()),\n",
    "        float(precision_metric.compute()),\n",
    "        float(recall_metric.compute()),\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    train_loss = train()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {train_loss:.4f}')\n",
    "    val_map, val_precision, val_recall = test(\n",
    "        test_edge_label_index,\n",
    "        test_exclude_links,\n",
    "    )\n",
    "    print(f'Test MAP@{k}: {val_map:.4f}, '\n",
    "          f'Test Precision@{k}: {val_precision:.4f}, '\n",
    "          f'Test Recall@{k}: {val_recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
